{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#First Part: What is Style Transfer?\n",
        "\n",
        "![image](https://drive.google.com/uc?export=view&id=1CyF8m1l-tVChLZmAzqLEKOwp8HbhEMB9)\n",
        "\n",
        "## This is style transfer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OFpSvGPIEHlr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22WKaJII_F5u"
      },
      "source": [
        "# Arbitrary Style Transfer in a data paucity scenario\n",
        "\n",
        "The objective of this project is to test the validity of the 'Adaptive instance normalization' (AdaIn) to capture style from images in the style transfer task.   \n",
        "The first approach was attempted in 2017 by [Xun Huang, Serge Belongiein](https://arxiv.org/abs/1703.06868), and it has proven to be successful and comparable with the previous non-arbitrary approach.  \n",
        "In the following years have been developed several strategies to carry out this particular task, including the more popular GANs (Generative Adversarial Networks) architectures.  \n",
        "One of the objective of the authors was to try new Encoder-Decoder architectures, more complex than vgg19, but still with the AdaIn mechanism.  \n",
        "In this notebook is provided a whole unofficial re-implementation with pytorch, of the main experiments conducted in the original paper, and was tested the effectiveness of AdaIn with Resnet34 with and without the residual connections.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sADDeLTVlNxg"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R8v2U912lPXb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import plotly.graph_objects as go\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import ImageFile\n",
        "from architectures import *\n",
        "from utils import *\n",
        "import ipywidgets as widgets\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "Image.MAX_IMAGE_PIXELS = 100000000000 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uQYdzr_9EcaW",
        "outputId": "a66a70c8-7bf6-4e90-811f-1ccb94a30019"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOeYFpT1EgZ1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "902IxjfuzA0Z"
      },
      "outputs": [],
      "source": [
        "!pip3 install pytorch-lightning==1.5.10\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "SEED = 2005\n",
        "\n",
        "pl.seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets\n",
        "\n",
        "* In order to make the Network to learn we need to feed it with two different datasets, one with the content images, and the other with the paintings.  \n",
        "\n",
        "* The former was the MS-COCO 2017, and the latter was the Painter by Numbers dataset (instead of the Wiki-Art dataset).  \n",
        "\n",
        "* In the experiments were used 20000 pairs of images from both datasets, whereas for the testing step were used 200 pairs of images. The datasets are available at https://drive.google.com/drive/folders/1S0S-H_vXYiKBR6lBbf46YSmbrBQfU1Yv?usp=sharing.  \n",
        "  \n",
        "* All the images were preprocessed according to the procedures described in the research. In particular the images were first resized to the 512x512 resolution and then were Center cropped to 256x256 resolution.\n",
        "\n",
        "* It was not necessary to normalize the images because the network was trained to do that, and the normalization would have affected the performances.\n",
        "\n",
        "![image](https://drive.google.com/uc?export=view&id=19bZ-IhozbQTnAUZU0IuSoj5Wn6w_TCkJ)\n"
      ],
      "metadata": {
        "id": "3uy-PnVIbesN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The next cells would work only if are defined the datasets' paths.\n",
        "* It was used pytorch lightning framework to develop the code in a more organised way."
      ],
      "metadata": {
        "id": "q8abzqgWenY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mQWFaYQDkpR"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize((512,512)),\n",
        "                               transforms.CenterCrop(256),\n",
        "                               transforms.ToTensor()])\n",
        "\n",
        "path_content = ''  # MS-COCO 20K Images\n",
        "path_style = ''    # Painter by Numbers 20k Images\n",
        "path_test_content = '' #MS-COCO 200 Images\n",
        "path_test_style = ''   #Painter by Numbers 200 Images\n",
        "\n",
        "# mscoco = torchvision.datasets.ImageFolder(root = path_content, transform = transform)\n",
        "# paint_by = torchvision.datasets.ImageFolder(root = path_style, transform = transform)\n",
        "# test_content = torchvision.datasets.ImageFolder(root = path_test_content, transform = transform)\n",
        "# test_style = torchvision.datasets.ImageFolder(root = path_test_style, transform = transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAAbtOH4vGnp"
      },
      "outputs": [],
      "source": [
        "class datasets(Dataset):  # This class is simply needed to inherit the methods from the Dataset class.\n",
        "\n",
        "  def __init__(self, dataset1, dataset2, dataset_length):\n",
        "    self.dataset1 = dataset1\n",
        "    self.dataset2 = dataset2\n",
        "    self.dataset_length = dataset_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.dataset_length\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    out1, _ = self.dataset1[idx]\n",
        "    out2, _ = self.dataset2[idx]\n",
        "    return out1,out2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfwfogCP0FZa"
      },
      "outputs": [],
      "source": [
        "# The lightning module is needed to combine the Datasets method with the dataloader functions #\n",
        "\n",
        "class pl_Datasets(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, dataset1, dataset2, dataset3, dataset4, batch_size, dataset_length):\n",
        "      self.d1 = dataset1\n",
        "      self.d2 = dataset2\n",
        "      self.d3 = dataset3\n",
        "      self.d4 = dataset4\n",
        "      self.batch_size = batch_size\n",
        "      self.dataset_length = dataset_length\n",
        "      self.test_set_length = 200\n",
        "    def setup(self, stage = None):\n",
        "        if stage == 'fit':\n",
        "            self.train_dataset = datasets(self.d1, self.d2, self.dataset_length) # These are referred to the main datasets 20k images from both\n",
        "        elif stage == 'test':\n",
        "            self.test_dataset = datasets(self.d3, self.d4, self.test_set_length) # These will be the other datasets\n",
        "\n",
        "    def train_dataloader(self, *args, **kwargs):\n",
        "        return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = True)\n",
        "\n",
        "    def val_dataloader(self, *args, **kwargs):\n",
        "        return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle = False)\n",
        "\n",
        "    def test_dataloader(self, *args, **kwargs):\n",
        "        return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle = False)\n",
        "\n",
        "batch_size = 8  # As specified in the paper\n",
        "dataset_length = 20000 #00\n",
        "universal_device = 'cuda'\n",
        "#pl_data = pl_Datasets(mscoco, paint_by, test_content, test_style, batch_size, dataset_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbNmqWW35kPd",
        "outputId": "37f3dce7-9ffc-4b1c-bec2-78087ea81b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the style image is torch.Size([8, 3, 256, 256])\n",
            "The size of the content image is torch.Size([8, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "pl_data.setup('fit')\n",
        "pl_data.setup('test')\n",
        "\n",
        "\n",
        "batch = next(iter(pl_data.train_dataloader()))\n",
        "content_image, style_image = batch\n",
        "print(\"The size of the style image is\",style_image.shape)\n",
        "print(\"The size of the content image is\",content_image.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture\n",
        "The heart of the method is the Adaptive Instance Normalization layer (AdaIn).  \n",
        " Such layer is defined as follows:  \n",
        "$$ AdaIN(x,y) = {\\sigma(y) \\cdot (\\frac{x- \\mu(x)}{\\sigma(x)}) + \\mu(y) }, $$ \n",
        "  \n",
        "where x is the Encoded Content Image, and y is the Encoded Style Image.\n",
        "\n",
        "### The Inthuition\n",
        "The inthuition behind this is that we embed the two images with the same pre-trained Encoder, and the AdaIn layer perform the images matching at a lower dimension. \n",
        "Then it follows a trained Decoder that will map back the feature maps to the image space. \n",
        "\n",
        "* Roughly speaking the AdaIn layer matches the stylistic features with the content features at a lower dimension, and then the reconstructed images will depict these stylistic features.\n",
        "\n",
        "* The reason why the layer is called adaptive is because it does not require to learn any parameter, but it depends always on the feature maps statistics. \n",
        "\n",
        "* Mean and variance (for each channel) of the content input (encoded) are adjusted to match those of the style input (encoded) ('Content' of the image is scaled by $\\sigma(y)$ and shifted by $\\mu(y)$).\n",
        "\n",
        "* In the decoder there are not any normalization layers, because they tend to center the content input to some pre-defined styles.  \n",
        "  \n",
        "All these information can be better understood by looking at the visual representation of the architecture. (Taken from the paper)  \n",
        "\n",
        "![image](https://drive.google.com/uc?export=view&id=1uiyj5xu62dAmbF02zNUKtfdN78y3ANTD)  \n",
        "As we can see from the image it was used a pre-trained Vgg19 as Encoder, the encoding was defined up to Relu 4_1 (512, 64x64).  \n",
        "* In my experiments i have used always this Encoder  \n",
        "* Together with the encoder was employed also a Decoder architecture, that was the one trained to reconstruct the image, in particular i have tried 3 different possible Decoder architectures:  \n",
        " 1. Vgg19,\n",
        " 2. ResNet34,\n",
        " 3. ResNet34 with no residual blocks. \n",
        "\n",
        "The (2) and (3) architecture are depicted in the following Figure:\n",
        "![image](https://drive.google.com/uc?export=view&id=1r5ZcaNfzQKch2J-GziKklXKYcy3l4M-V)  \n",
        "In the (3), we do not consider the residual connections.\n",
        "\n",
        "Additionally the main network is fully convolutional, thus we can apply it to images of any size.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SQUStA6-fkM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbTAqWOrrVml"
      },
      "outputs": [],
      "source": [
        "# This is the engineering code, here we defining the architecture of the AdaIn network, also using the Encoder and Decoder defineed in the architectures.py file in the repository. #\n",
        "\n",
        "class Neural_style_network(pl.LightningModule):\n",
        "  def __init__(self, lr, dec_path, alpha, num_epochs, device, first_train = False, net = 'vgg', residuals = True):\n",
        "    super(Neural_style_network,self).__init__()\n",
        "    \n",
        "    self.enc = Encoder(device) # The Encoder is common to all the networks.\n",
        "    self.path = dec_path       # The path is chosen to be where it was saved the model, or where we are about to save the model.\n",
        "    if net == 'vgg':\n",
        "      self.dec = Decoder()     # The Decoder is the mirror of the Encoder\n",
        "    elif net == 'res':\n",
        "      self.dec = DecodedRes(residuals) # The Decoder for resnet is different, so it was defined as a different class, and it alsoo change based on the presence of the residuals.\n",
        "\n",
        "    self.first = first_train           # We need to specify if it is the first trial with a network or not\n",
        "    self.lr = lr\n",
        "    self.alpha = alpha                 # alpha is the style weight to apply in the total loss function.\n",
        "    self.epochs = num_epochs           # These are the number of epochs\n",
        "    self.loss_list = []\n",
        "    self.optimizer = torch.optim.Adam(self.dec.parameters(), lr=self.lr)\n",
        "    self.scheduler = lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.epochs, eta_min= 0, last_epoch= -1, verbose=True)\n",
        "    \n",
        "    if first_train!=True:\n",
        "      self.checkpoint = load_model(self.path, self, device)\n",
        "      self.scheduler.load_state_dict(self.checkpoint['scheduler'])\n",
        "\n",
        "    \n",
        "  \n",
        "  def forward(self, content_image, style_image, test = None):  #This method implements the architecture as sketched in the structure.\n",
        "    enc_image = self.enc.forward(content_image)\n",
        "    enc_style = self.enc.forward(style_image, lista = True)  # The list that we give to the encoder means that we want to retrieve from the layers of the networks the levels associated to relu_1_1, relu_2_1, relu_3_1, relu_4_1.\n",
        "\n",
        "    adapted_image = self.AdaIn(enc_image, enc_style[-1])\n",
        "    \n",
        "    if test!=None: # At test time we want to return the decoded image\n",
        "      \n",
        "      decoded_adapt =(1-test)*enc_image +test*(adapted_image) # We can decide the level of styleness to apply at the decoded image.\n",
        "      \n",
        "      return self.dec(decoded_adapt)\n",
        "    \n",
        "    decoded_adapt =  self.dec(adapted_image)\n",
        "    renc = self.enc.forward(decoded_adapt, lista = True)\n",
        "\n",
        "    content_loss = self.Content_loss(renc[-1], adapted_image)\n",
        "    style_loss = self.Style_loss(renc, enc_style)  \n",
        "\n",
        "    return self.total_loss(content_loss, style_loss)\n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    content, style = batch\n",
        "    loss = self.forward(content, style)\n",
        "    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    content, style = batch\n",
        "    loss = self.forward(content, style)\n",
        "    self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    self.loss_list.append(loss.item())\n",
        "    return loss\n",
        "\n",
        "  def on_train_epoch_end(self, *args, **kwargs):\n",
        "    if self.path!=None:  # If the target PATH is not defined we don't save the model.\n",
        "      model = self.state_dict()\n",
        "      loss = sum(self.loss_list)/len(self.loss_list)\n",
        "      checkpoint = {}\n",
        "      checkpoint['model_state'] = model\n",
        "      checkpoint['scheduler'] = self.scheduler.state_dict()\n",
        "      if self.first: # we initialize the checkpoint loss list\n",
        "        checkpoint['loss'] = [loss]\n",
        "        self.first = False # Now we know that for a given path there will be already some information stored. \n",
        "\n",
        "      else:\n",
        "        checkpoint['loss'] = self.checkpoint['loss'] + [loss] # Update the previous information.\n",
        "\n",
        "      self.loss_list = [] # re-initialize the loss before the next epoch\n",
        "      save_model(checkpoint, self.path)\n",
        "      self.checkpoint = load_model(self.path, self, self.device)\n",
        "      self.scheduler.load_state_dict(self.checkpoint['scheduler'])\n",
        "    else:\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return [self.optimizer],[self.scheduler]\n",
        "\n",
        "  def calc_mean_std(self, input, eps=1e-5): # given a feature maps layer, for each channel and each batch we compute its mean and variance (batch, channel, 1, 1)\n",
        "    batch_size, channels = input.shape[:2]\n",
        "\n",
        "    reshaped = input.view(batch_size, channels, -1) # Reshape channel wise\n",
        "    mean = torch.mean(reshaped, dim = 2).view(batch_size, channels, 1, 1) # Calculate mean and reshape\n",
        "    std = torch.sqrt(torch.var(reshaped, dim=2)+eps).view(batch_size, channels, 1, 1) # Calculate variance, add epsilon (avoid 0 division),\n",
        "                                                                                      # calculate std and reshape\n",
        "    return mean, std\n",
        "\n",
        "  def total_loss(self, content_loss, style_loss): # This is the total loss\n",
        "    return content_loss + self.alpha*style_loss\n",
        "\n",
        "\n",
        "  def AdaIn(self, content, style):\n",
        "    assert content.shape[:2] == style.shape[:2] # Only first two dim, such that different image sizes is possible\n",
        "    batch_size, n_channels = content.shape[:2]\n",
        "    mean_content, std_content = self.calc_mean_std(content)\n",
        "    mean_style, std_style = self.calc_mean_std(style)\n",
        "\n",
        "    output = std_style*((content - mean_content) / (std_content)) + mean_style # Normalize, then modify mean and std\n",
        "    return output\n",
        "\n",
        "  def Content_loss(self, input, target): # Content loss is a simple MSE Loss, we want to reduce the distance of the AdaIn output, with the re-encoded stylized image\n",
        "    loss = F.mse_loss(input, target)\n",
        "    return loss\n",
        "\n",
        "  def Style_loss(self, input, target):\n",
        "    mean_loss, std_loss = 0, 0\n",
        "\n",
        "    for input_layer, target_layer in zip(input, target): \n",
        "      mean_input_layer, std_input_layer = self.calc_mean_std(input_layer)\n",
        "      mean_target_layer, std_target_layer = self.calc_mean_std(target_layer)\n",
        "\n",
        "      mean_loss += F.mse_loss(mean_input_layer, mean_target_layer) # Distance in the same channels is reduced within the same layer, and then it is done for all the layers.\n",
        "      std_loss += F.mse_loss(std_input_layer, std_target_layer)\n",
        "\n",
        "    return mean_loss+std_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training step \n",
        "\n",
        "The networks were trained until their test loss started to become too flat.\n",
        "Some of the hyperparameters are listed here:  \n",
        "* learning rate: $10^{-5}$,\n",
        "* λ: 2, (This is different from the weight decay!!)\n",
        "* batch size: 8,\n",
        "* [Adam Optimizer](https://arxiv.org/abs/1412.6980)\n",
        "* [Cosine learning rate Scheduler](https://arxiv.org/abs/1608.03983)\n",
        "* Up-Sampling mode: 'nearest'\n",
        "* Reflection Padding.  \n",
        "The implementation of the losses were taken from this [repository](https://github.com/MAlberts99/PyTorch-AdaIN-StyleTransfer).  \n",
        "The formulas are:  \n",
        "$$ \\text{Content Loss}: L_c = {|| f(g(t)) - t ||_2}, $$ \n",
        "$$ \\text{Style Loss}: L_s = {\\sum_{i=1}^{L} || \\mu(ϕ_i(g(t))) - \\mu(ϕ_i(s)) ||_2 + || \\sigma(ϕ_i(g(t))) - \\sigma(ϕ_i(s)) ||}, $$\n",
        "$$ \\text{Total Loss}: L_c + λ\\cdot L_s. $$  \n",
        "\n",
        "Where $t = AdaIn(x,y)$, $g(t)$ is the decoded image, and f($\\cdot$) is the encoder function.  \n",
        "\n",
        "As regards to the Style Loss, we minimize only the distances between the statistics, whereas for the content loss we want to preserve the spatial structure of the image.\n",
        "\n"
      ],
      "metadata": {
        "id": "sEZMjpAgmOZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdRxOqLpfFOn"
      },
      "outputs": [],
      "source": [
        "lr = 0.00001\n",
        "alpha = 2\n",
        "num_epochs =20\n",
        "\n",
        "net_type = 'vgg'\n",
        "\n",
        "universal_device = 'cuda'\n",
        "\n",
        "PATH = None #'./models/' + net_type+ \".pt\", if None, the model won't be saved.\n",
        "\n",
        "FIRST = False # If false we don't load any path, but at the end of every epoch we save the model at the initialized path.\n",
        "\n",
        "model = Neural_style_network( lr, PATH, alpha, num_epochs, universal_device, first_train = FIRST) #, net = 'res', residuals = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz15EmcifFNy"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=num_epochs,  # maximum number of epochs.\n",
        "    gpus=1,  # the number of gpus we have at our disposal.\n",
        "    default_root_dir=\"./models/\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNdrjedZM-Ij"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model = model, datamodule = pl_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of the training code\n",
        "### Proceed with the other notebook......"
      ],
      "metadata": {
        "id": "ytzdXoPMvpye"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DataPaucity_StyleTransfer.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}